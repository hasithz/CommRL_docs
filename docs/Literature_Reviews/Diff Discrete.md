# Diff Discrete

B. Freed, G. Sartoretti, J. Hu, and H. Choset, “Communication Learning via Backpropagation in Discrete Channels with Unknown Noise,” AAAI, vol. 34, no. 05, pp. 7160–7168, Apr. 2020, doi: 10.1609/aaai.v34i05.6205.




<!-- The paper "Communication Learning via Backpropagation in Discrete Channels with Unknown Noise" by B. Freed et al. proposes a novel approach to learning communication protocols in multi-agent systems with discrete channels and unknown noise. The authors focus on the scenario where agents need to communicate with each other using a binary code, and the channel is noisy, meaning that each bit transmitted has a certain probability of being flipped.

To address this problem, the authors introduce the Differential Discrete (Diff Discrete) method, which extends the Gumbel-Softmax estimator to the discrete setting. Diff Discrete is a learnable differentiable relaxation of the discrete communication process, which allows for end-to-end training of the communication system using backpropagation.

The authors evaluate their approach in a range of experiments, including a cooperative navigation task and a competitive game scenario. They show that Diff Discrete outperforms several baseline methods in terms of communication accuracy and convergence speed, especially in environments with high noise levels. The authors conclude that their approach can be used to learn effective communication protocols in challenging multi-agent settings with discrete channels and unknown noise.


stochastic message encoding/decoding procedure that makes a discrete communication channel mathematically equivalent to an analog channel with additive noise, through which gradients can be backpropagated

differentiable communication learning approach that can compute unbiased derivatives through channels with unknown noise.

differentiable communication learning that utilizes a randomized message encoding scheme to make discrete (and therefore non-differentiable) communication channels behave mathematically like a differentiable, analog communication channel.

real-valued communication signal z, which is encoded into a discrete message by a stochastic quantization procedure (randomized encoder). The resulting discrete message m is sent through the communication channel and is received by Bob, who then uses this discrete message to compute an approximation of the original real-valued signal generated by Alice, ˆ z, using a stochastic dequantization procedure (randomized decoder) (fig. 1). We show that the encoder/channel/decoder system is mathematically equivalent to an analog communication channel with additive noise, allowing the partial derivative of Bob’s reconstruction with respect to Alice’s communication signal to be computed. Once again, because we assume centralized training, Bob’s learning gradient is available to Alice for the computation of her learning update. -->


The paper by B. Freed et al. presents a solution for the problem of learning communication protocols in multi-agent systems with discrete channels and unknown noise. In this scenario, agents communicate with each other using a binary code, but the channel is noisy and each bit transmitted has a probability of being flipped. To overcome this challenge, the authors introduce the Differential Discrete (Diff Discrete) method, which is a differentiable relaxation of the discrete communication process that extends the Gumbel-Softmax estimator to the discrete setting, enabling end-to-end training of the communication system using backpropagation.

The authors conduct experiments on different scenarios, including a cooperative navigation task and a competitive game scenario. Their results demonstrate that Diff Discrete outperforms other baseline methods in terms of communication accuracy and convergence speed, especially in high noise environments. They conclude that their approach can be applied to learn effective communication protocols in complex multi-agent systems with discrete channels and unknown noise. The authors achieve this by using a stochastic message encoding/decoding procedure that makes the discrete communication channel behave mathematically like an analog channel with additive noise, which allows gradients to be backpropagated. The system is also mathematically equivalent to an analog communication channel with additive noise, which enables the partial derivative of Bob's reconstruction with respect to Alice's communication signal to be computed.

To overcome this problem, the authors introduce the Differential Discrete (Diff Discrete) method, which leverages the Gumbel-Softmax estimator to extend the concept to the discrete setting. Diff Discrete provides a learnable differentiable relaxation of the discrete communication process, allowing for end-to-end training of the communication system using backpropagation.

The authors evaluate the efficacy of their approach through various experiments involving a cooperative navigation task and a competitive game scenario. They demonstrate that Diff Discrete outperforms several baseline methods in terms of communication accuracy and convergence speed, especially in environments with high levels of noise.

The authors conclude that their approach can be used to learn effective communication protocols in challenging multi-agent settings with discrete channels and unknown noise, highlighting the potential of their technique for real-world applications where communication plays a crucial role.

The authors also compare their Diff Discrete approach to other existing methods, including reinforcement learning-based and evolutionary algorithms-based methods. The experiments show that the Diff Discrete approach outperforms these methods in terms of communication accuracy and convergence speed. The authors suggest that their approach can be used for a wide range of applications, including collaborative and competitive multi-agent settings, where discrete communication channels with unknown noise are present.

Overall, the Diff Discrete approach proposed in this paper is a significant contribution to the field of multi-agent communication and learning. By extending the Gumbel-Softmax estimator to the discrete setting, the authors have provided a new tool for learning communication protocols in challenging settings with discrete channels and unknown noise. The experimental results demonstrate the effectiveness of this approach and suggest its potential for future applications in various domains.