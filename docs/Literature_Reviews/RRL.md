# RRL (Relational Deep Reinforcement Learning)

V. Zambaldi et al., “Relational Deep Reinforcement Learning.” arXiv, Jun. 28, 2018. Accessed: Apr. 15, 2023. [Online]. Available: http://arxiv.org/abs/1806.01830

<!-- 

The core idea behind RRL is to combine reinforcement learning with relational learning or Inductive Logic Programming [16] by representing states, actions and policies using a first order (or relational) language [8, 9, 17, 18]. Moving from a propositional to a relational representation facilitates generalization over goals, states, and actions, exploiting knowledge learnt during an earlier learning phase. Additionally, a relational language also facilitates the use of background knowledge. Background knowledge can be provided by logical facts and rules relevant to the learning problem. For example in a blocks world, one could use the predicate above(S, A, B) to indicate that block A is above block B in state S when specifying background knowledge. Such predicates can then be used during learning for blocks C and D, for example. The representational language, background, and assumptions form the inductive bias, which guides (and restricts) the search for good policies. The language (or declarative) bias determines the way concepts can be represented. Neural nets have traditionally been associated with the attribute-value, or propositional, RL approaches [19]. Here we translate ideas from RRL into architecturally specified inductive biases within a deep RL agent, using neural network models that operate on structured representations of a scene – sets of entities – and perform relational reasoning via iterated, message-passing-like modes of processing. The entities correspond to local regions of an image, and the agent learns to attend to key objects and compute their pairwise and higher-order interactions.

We equip a deep RL agent with architectural inductive biases that may be better suited for learning (and computing) relations, rather than specifying them as background knowledge as in RRL. This approach builds off previous work suggesting that relational computations needn’t necessarily be biased by entities’ spatial proximity [20, 10, 21, 11, 13, 22], and may also profit from iterative structured reasoning [23, 24, 25, 26]. Our contribution is founded on two guiding principles: non-local computations using a shared function and iterative computation. We show that an agent which computes pairwise interactions between entities, independent of their spatial proximity, using a shared function, will be better suited for learning important relations than an agent that only computes local interactions, such as in translation invariant convolutions1. Moreover, an iterative computation may be better able to capture higher-order interactions between entities.

Computing non-local interactions using a shared function 
Among a family of related approaches for computing non-local interactions [20], we chose a computationally efficient attention mechanism. This mechanism has parallels with graph neural networks and, more generally, message passing computations [27, 28, 29, 12, 30]. In these models entity-entity relations are explicitly computed when considering the messages passed between connected nodes of the graph.

We start by assuming that we already have a set of entities for which interactions must be computed. We consider multi-head dot-product attention (MHDPA), or self-attention [14], as the operation that computes interactions between these entities.

For N entities (e1:N ), MHDPA projects each entity i’s state vector, ei, into query, key, and value vector representations: qi, ki, vi, respectively, whose activities are subsequently normalized to have 0 mean and unit variance using the method from [31]. Each qi is compared to all entities’ keys k1:N via a dot-product, to compute unnormalized saliencies, si. These are normalized into weights, wi = softmax (si). For each entity, the cumulative interactions are computed by the weighted mixture of all entities’ value vectors, ai = ∑ j=1:N wi,jvj. This can be compactly computed using matrix multiplications

where A, Q, K, and V compile the cumulative interactions, queries, keys, and values into matrices, and d is the dimensionality of the key vectors used as a scaling factor. Like [14], we use multiple, independent attention “heads”, applied in parallel, which our attention visualisation analyses (see Results 4.1) suggest may assume different relational semantics through training. The ah i vectors, where h indexes the head, are concatenated together, passed to a multilayer perceptron (2-layer MLP with ReLU non-linearities) with the same layers sizes as ei, summed with ei (i.e., a residual connection), and transformed via layer normalization [31], to produce an output. Figure 2 depicts this mechanism. We refer to one application of this process as an “attention block”. A single block performs non-local pairwise relational computations, analogous to relation networks [13] and non-local neural networks [20]. Multiple blocks with shared (recurrent) or unshared (deep) parameters can be composed to more easily approximate higher order relations, analogous to message-passing on graphs. -->

![RRL architecture as in paper](https://raw.githubusercontent.com/hasithz/CommRL_docs/efc4fec7e688817315a67ef1d5d5efd89a5802be/assets/images/RRL.drawio.svg)

In the paper "Relational Deep Reinforcement Learning" by V. Zambaldi et al., the authors propose a new approach to combine reinforcement learning with relational learning, also known as Inductive Logic Programming. They do this by introducing an architectural inductive bias within a deep RL agent using neural network models that operate on structured representations of a scene. This approach is based on two guiding principles: non-local computations using a shared function and iterative computation.

The core idea is to facilitate generalization over goals, states, and actions by representing states, actions, and policies using a first-order (or relational) language. This approach not only aids in generalization but also allows for the use of background knowledge, which can be provided by logical facts and rules relevant to the learning problem.

The authors propose computing non-local interactions using a shared function, specifically a computationally efficient attention mechanism, which has parallels with graph neural networks and message-passing computations. They use multi-head dot-product attention (MHDPA), or self-attention, as the operation that computes interactions between entities.

In this architecture, a single attention block performs non-local pairwise relational computations, analogous to relation networks and non-local neural networks. By composing multiple blocks with shared (recurrent) or unshared (deep) parameters, higher-order relations can be more easily approximated, analogous to message-passing on graphs.

In summary, the authors present a novel approach to deep reinforcement learning that leverages relational learning and inductive biases within a deep RL agent. This enables the agent to learn better relations and generalize over goals, states, and actions, which has potential applications in various complex learning problems.